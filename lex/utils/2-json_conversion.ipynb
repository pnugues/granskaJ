{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Conversion\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granska's lexical resources are stored in a slightly inconsistent TSV format. In this notebook, we convert them in the more maintainable JSON format without altering their structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex as re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The resource names and locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source and destination folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_folder = '../../lex/'\n",
    "dest_folder = '../../lex/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `morfs` folder and its files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morfs_folder = 'morfs/'\n",
    "morfs_files = ['cw', 'cwt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tags` folder and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_folder = 'tags/'\n",
    "tags_files = ['ct', 'ctm', 'ctt', 'cttt', 'features', 'taginfo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `word` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_folder = 'words/'\n",
    "words_files = ['bitransitivaverb', 'compound-begin-ok.w', 'compound-end-stop.w',\n",
    "              'cw', 'cwtl', 'feminina', 'foreign.w', 'inflection.lex', 'inflection.rules',\n",
    "              'intransitivaverb', 'opt_space_words', 'spellNotOK', 'spellOK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `morfs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the `morfs` files is easy. Each line has a number and one or two tokens. We store them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in morfs_files:\n",
    "    data = open(src_folder + morfs_folder + file, encoding='utf-8').read().strip()\n",
    "    lines = re.split('[\\r\\n]+', data)\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        fields = re.split('[\\t]+', line.strip())\n",
    "        fields = list(map(str.strip, fields))\n",
    "        fields[0] = int(fields[0])\n",
    "        new_lines += [fields]\n",
    "    new_lines = {file: new_lines}\n",
    "    fp = open(dest_folder + morfs_folder + file + '.json', 'w', encoding='utf-8')\n",
    "    json.dump(new_lines, fp, indent=2, ensure_ascii=False)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tags`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outside `features`, we have a flat file struture and we convert it as for `morfs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_files.remove('features')\n",
    "for file in tags_files:\n",
    "    data = open(src_folder + tags_folder + file, encoding='utf-8').read().strip()\n",
    "    lines = re.split('[\\r\\n]+', data)\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        fields = re.split('[ \\t]+', line.strip())\n",
    "        fields = list(map(str.strip, fields))\n",
    "        if file != 'taginfo':\n",
    "            fields[0] = int(fields[0])\n",
    "        new_lines += [fields]\n",
    "    new_lines = {file: new_lines}\n",
    "    fp = open(dest_folder + tags_folder + file + '.json', 'w', encoding='utf-8')\n",
    "    json.dump(new_lines, fp, indent=4, ensure_ascii=False)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `features` file has a two-level structure. We parse it with a two-pass procedure. In the first pass, we store the indices of the first level and in the second pass we process the second level. The first level starts with a `*` and corresponds to a feature category and its translation in Swedish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(src_folder + tags_folder + 'features', encoding='utf-8').read().strip()\n",
    "lines = re.split('[\\r\\n]+', data)\n",
    "first_level_idx = []\n",
    "for i, line in enumerate(lines):\n",
    "    if line[0] == '*':\n",
    "       first_level_idx += [i] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now process the second level that consists of pairs. We encode them as dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "for i, line in enumerate(lines):\n",
    "    fields = re.split('\\t+', line.strip())\n",
    "    fields = list(map(str.strip, fields))\n",
    "    if i in first_level_idx:\n",
    "        name = fields[0][1:]\n",
    "        features[name] = {}\n",
    "        pos_feat_name = name\n",
    "        features[name]['swedish_transl'] = fields[1]\n",
    "        features[name]['values'] = {}\n",
    "    else:\n",
    "        features[pos_feat_name]['values'][fields[0]] = fields[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we store the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(dest_folder + tags_folder + 'features.json', 'w', encoding='utf-8')\n",
    "json.dump(features, fp, indent=2, ensure_ascii=False)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `words`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this folder, some files are just lists of words or phrases. We encode them as JSON lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_files = ['bitransitivaverb', 'compound-begin-ok.w', 'compound-end-stop.w',\n",
    "              'feminina', 'foreign.w', 'intransitivaverb', 'opt_space_words', 'spellNotOK', 'spellOK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in words_files:\n",
    "    data = open(src_folder + words_folder + file, encoding='utf-8').read().strip()\n",
    "    lines = re.split('[\\r\\n]+', data)\n",
    "    new_lines = [line.strip() for line in lines]\n",
    "    new_lines = {file: new_lines}\n",
    "    fp = open(dest_folder + words_folder + file + '.json', 'w', encoding='utf-8')\n",
    "    json.dump(new_lines, fp, indent=2, ensure_ascii=False)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three other files have fields separated by tabulations. We encode them as a list of li.. For these files, a few lines need to be corrected manually as the original files, sometimes use tabulations as separator and sometimes, spaces. There is also a character that is not in the Latin 1 register."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_files = ['cw', 'cwtl', 'inflection.lex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in words_files:\n",
    "    data = open(src_folder + words_folder + file, encoding='utf-8').read().strip()\n",
    "    lines = re.split('[\\r\\n]+', data)\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        fields = re.split('[ \\t]+', line.strip())\n",
    "        fields = list(map(str.strip, fields))\n",
    "        if file != 'inflection.lex':\n",
    "            fields[0] = int(fields[0])\n",
    "        new_lines += [fields]\n",
    "    new_lines = {file: new_lines}\n",
    "    fp = open(dest_folder + words_folder + file + '.json', 'w', encoding='utf-8')\n",
    "    json.dump(new_lines, fp, indent=2, ensure_ascii=False)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally `inflection.rules`has a two level-structure that we convert with a two-pass procedure. The first-level lines start with a `$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(src_folder + words_folder + 'inflection.rules', encoding='utf-8').read().strip()\n",
    "lines = re.split('[\\r\\n]+', data)\n",
    "first_level_idx = []\n",
    "for i, line in enumerate(lines):\n",
    "    if line[0] == '$':\n",
    "       first_level_idx += [i] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the second level. We create dictionaries, where the keys will be pair of paradigm and suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_list = []\n",
    "for i, line in enumerate(lines):\n",
    "    fields = re.split('\\t+', line.strip())\n",
    "    fields = list(map(str.strip, fields))\n",
    "    if i in first_level_idx:\n",
    "        if i != 0:\n",
    "            rule_list += [copy.deepcopy(new_dict)]\n",
    "        new_dict = {}\n",
    "        fields[0] = fields[0][1:]\n",
    "        new_dict['feat_infl'] = fields[:]\n",
    "        new_dict['paradigm'] = []\n",
    "    else:\n",
    "        new_dict['paradigm'] += [fields[:]]\n",
    "rule_list += [copy.deepcopy(new_dict)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the top level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_rules = {}\n",
    "inflection_rules['inflection.rules'] = rule_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we store the rules in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(dest_folder + words_folder + 'inflection.rules.json', 'w', encoding='utf-8')\n",
    "json.dump(inflection_rules, fp, indent=2, ensure_ascii=False)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
